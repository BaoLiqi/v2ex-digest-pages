{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# V2EX Post Analysis Notebook\n",
        "\n",
        "This notebook processes V2EX post data by applying LLM-based analysis to each chunk of text within the JSON files. It adds the analysis results to each chunk and writes the modified data to new JSON files.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, we'll install the required dependencies and set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_files_header"
      },
      "source": [
        "## Download Files\n",
        "\n",
        "Next, we'll download the analysis script and the JSON files from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_files"
      },
      "outputs": [],
      "source": [
        "# Clone the repository to get the JSON files and script\n",
        "!git clone https://github.com/yourusername/v2ex-digest-pages.git\n",
        "\n",
        "# Change to the repository directory\n",
        "%cd v2ex-digest-pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "check_files_header"
      },
      "source": [
        "## Check Files\n",
        "\n",
        "Let's check that we have the necessary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_files"
      },
      "outputs": [],
      "source": [
        "# Check that we have the analysis script\n",
        "!ls -la analyze_posts.py\n",
        "\n",
        "# Check that we have the JSON files\n",
        "!ls -la docs/posts_json/ | head -n 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_analysis_header"
      },
      "source": [
        "## Run Analysis\n",
        "\n",
        "Now we'll run the analysis script to process the JSON files. We'll limit the number of files to process to avoid running out of memory or time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_analysis"
      },
      "outputs": [],
      "source": [
        "# Run the analysis script with a limit of 5 files\n",
        "# You can adjust the limit based on your available resources\n",
        "!python analyze_posts.py --limit 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "check_results_header"
      },
      "source": [
        "## Check Results\n",
        "\n",
        "Let's check the results to make sure the analysis was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_results"
      },
      "outputs": [],
      "source": [
        "# Check that we have the output files\n",
        "!ls -la docs/posts_json_analyzed/ | head -n 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examine_results_header"
      },
      "source": [
        "## Examine Results\n",
        "\n",
        "Let's examine one of the output files to see the analysis results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "examine_results"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Get the first output file\n",
        "output_dir = \"docs/posts_json_analyzed\"\n",
        "output_files = [f for f in os.listdir(output_dir) if f.endswith(\"_analyzed.json\")]\n",
        "if output_files:\n",
        "    output_file = os.path.join(output_dir, output_files[0])\n",
        "    print(f\"Examining file: {output_file}\")\n",
        "    \n",
        "    # Load the file\n",
        "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Print the first chunk with analysis\n",
        "    if data[\"blocks\"] and data[\"blocks\"][0][\"chunks\"]:\n",
        "        chunk = data[\"blocks\"][0][\"chunks\"][0]\n",
        "        print(\"English text:\", chunk[\"en\"])\n",
        "        print(\"Chinese text:\", chunk[\"zh\"])\n",
        "        print(\"\\nAnalysis:\")\n",
        "        if \"analysis\" in chunk:\n",
        "            # Print the first few tokens and their top candidates\n",
        "            for i, token_analysis in enumerate(chunk[\"analysis\"][\"analysis\"][:5]):\n",
        "                print(f\"Token {i+1}: {token_analysis['token']}\")\n",
        "                print(\"Top candidates:\")\n",
        "                for j, candidate in enumerate(token_analysis[\"candidates\"][:3]):\n",
        "                    print(f\"  {j+1}. {candidate['token']} ({candidate['probability']})\")\n",
        "                print()\n",
        "        else:\n",
        "            print(\"No analysis found for this chunk.\")\n",
        "else:\n",
        "    print(\"No output files found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_results_header"
      },
      "source": [
        "## Download Results\n",
        "\n",
        "Finally, let's download the results to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "# Create a zip file of the results\n",
        "!zip -r posts_json_analyzed.zip docs/posts_json_analyzed/\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('posts_json_analyzed.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has processed the V2EX post data by applying LLM-based analysis to each chunk of text within the JSON files. The analysis results have been added to each chunk and the modified data has been written to new JSON files.\n",
        "\n",
        "You can adjust the parameters of the analysis script to process more files or use a different model by modifying the command in the \"Run Analysis\" cell. For example, to process all files, remove the `--limit` parameter:\n",
        "\n",
        "```python\n",
        "!python analyze_posts.py\n",
        "```\n",
        "\n",
        "Or to use a different model:\n",
        "\n",
        "```python\n",
        "!python analyze_posts.py --model_name \"different/model\"\n",
        "```\n",
        "\n",
        "Note that processing all files may take a long time and require significant resources."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
