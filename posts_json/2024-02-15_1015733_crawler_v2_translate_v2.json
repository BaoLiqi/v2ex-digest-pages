{
  "id": 1015733,
  "blocks": [
    {
      "chunks": [
        {
          "zh": "超大型文件比较，内存不足，只能分页读区再匹配，但头都秃了，也没想到优化的方式，朋友们帮帮忙啊。",
          "en": "Comparing extremely large files, with insufficient memory, I can only read and match them page by page, but I'm pulling my hair out and haven't come up with an optimized solution. Friends, please help."
        }
      ],
      "type": "title"
    },
    {
      "chunks": [
        {
          "zh": "有两个本地文件，PersonListA.csv 和 PersonListB.csv ，文件过于巨大，我需要分批读取到内存里，变成 List<Person> A 和 List<Person> B ，我现在要把 A 和 B 进行对比，获得下面结果：",
          "en": "There are two local files, PersonListA.csv and PersonListB.csv. The files are too large; I need to read them into memory in batches, becoming List<Person> A and List<Person> B. I now need to compare A and B to get the following results:"
        },
        {
          "zh": "Person 有两个属性，一个 Name ，一个 Age ，Name 是唯一的，但两个列表中相同 Name 的人 Age 不一样",
          "en": "Person has two attributes, a Name and an Age. Name is unique, but people with the same Name in the two lists have different Ages"
        },
        {
          "zh": "1 ，找出存在于 A 但不存在于 B 的元素。",
          "en": "1, Find the elements that exist in A but not in B."
        },
        {
          "zh": "2 ，找出存在于 B 但不存在于 A 的元素。",
          "en": "2, Find the elements that exist in B but not in A."
        },
        {
          "zh": "3 ，找出两个列表中不同的元素。",
          "en": "3, Find the different elements in the two lists."
        },
        {
          "zh": "有以下限定条件：",
          "en": "There are the following constraints:"
        },
        {
          "zh": "1 ，本地文件过大，无法一次性读取到内存中，需要分页进行比较。",
          "en": "1, The local files are too large to be read into memory at once, requiring comparison by paging."
        },
        {
          "zh": "2 ，无法对两个列表进行排序，数据在列表中都是乱序的，需要用 Name 进行匹配。",
          "en": "2, It is not possible to sort the two lists; the data in the lists are in random order, requiring matching using Name."
        },
        {
          "zh": "如果要实现以上 function ，需要用到什么算法，请分析后给出算法的名称。",
          "en": "If you want to implement the above function, what algorithms do you need to use? Please analyze and provide the name of the algorithm."
        },
        {
          "zh": "现在的策略是，把 A 中的数据分为 10 组 10 条，然后把 B 中的数据分为 10 组 10 条的数据，分别拿 A 中的每一组数据和 B 中的每一组数据进行比较，这个方法叫做 GetUniqueResult",
          "en": "The current strategy is to divide the data in A into 10 groups of 10 items, and then divide the data in B into 10 groups of 10 items. Then, compare each group of data in A with each group of data in B. This method is called GetUniqueResult"
        },
        {
          "zh": "1 ，在找到只存在 A 不存在 B 的元素调用一次 GetUniqueResult 。",
          "en": "1, Call GetUniqueResult once when finding elements that exist only in A and not in B."
        },
        {
          "zh": "2 ，在找到只存在 B 不存在 A 的元素调用一次 GetUniqueResult 。",
          "en": "2, Call GetUniqueResult once when finding elements that exist only in B and not in A."
        },
        {
          "zh": "3 ，在通过 FindDiffer 找到 A 和 B 中不同的元素。",
          "en": "3, Find the different elements in A and B through FindDiffer."
        },
        {
          "zh": "相当于我需要循环 3*(10^10)次才能结束，其中也有使用 HashMap 来进行优化，使得实际执行次数小于 3*(10^10)次。（比如 A 中的第一组数据在和 B 的所有数据比较时，如果第一组的所有数据都找到了，就提前结束。）",
          "en": "This equates to needing to loop 3*(10^10) times to finish. HashMap is also being used for optimization, making the actual number of executions less than 3*(10^10) times. (For example, when the first group of data in A is compared with all the data in B, if all the data in the first group is found, the process ends early.)"
        },
        {
          "zh": "我现在想问的是，有没有什么更好的算法能从 3*(10^10) 优化到 (10^10)，如果能跟优化到 N （ 1 ）",
          "en": "What I want to ask now is whether there is a better algorithm to optimize from 3*(10^10) to (10^10), or even to N (1)"
        },
        {
          "zh": "有没有什么方法能只对调用一次 GetUniqueResult ，获得全部结果。",
          "en": "Is there any way to call GetUniqueResult only once to obtain all the results?"
        },
        {
          "zh": "-----------------------------------------------",
          "en": "-----------------------------------------------"
        },
        {
          "zh": "浓缩版：",
          "en": "Condensed version:"
        },
        {
          "zh": "我有两个本地 FileA.csv 和 FileB.csv ，分别存储了从 Oracle 中和 PostgreSQL 导出的同一个表。",
          "en": "I have two local files, FileA.csv and FileB.csv, which store the same table exported from Oracle and PostgreSQL, respectively."
        },
        {
          "zh": "1 ，单个文件超过 10GB ，无法一次性读取到内存中。",
          "en": "1, A single file exceeds 10GB and cannot be read into memory at once."
        },
        {
          "zh": "2 ，两个导出的文件是乱序的，需要用主键在程序里进行数据匹配（文件无法修改）。",
          "en": "2, The two exported files are in random order, and data matching needs to be done in the program using the primary key (the files cannot be modified)."
        },
        {
          "zh": "需要获取结果：",
          "en": "Need to get the results:"
        },
        {
          "zh": "1 ，在 A 中存在，但 B 中不存在的数据。",
          "en": "1, Data that exists in A but not in B."
        },
        {
          "zh": "2 ，在 B 中存在，但 A 中不存在的数据。",
          "en": "2, Data that exists in B but not in A."
        },
        {
          "zh": "3 ，在 A 和 B 中都存在，但有差异的数据。",
          "en": "3, Data that exists in both A and B but has differences."
        },
        {
          "zh": "可以有什么办法吗？或者给我一些算法关键字，比如我现在使用的是类哈希分页对比。",
          "en": "Are there any solutions? Or give me some algorithm keywords, for example, I am currently using a hash-like paging comparison."
        }
      ],
      "type": "content"
    },
    {
      "chunks": [
        {
          "zh": "@laike9m 因为就故意想要吃屎，有靠谱的简单解法不错，非得要屎上雕花。这玩意随便弄个数据库建个索引就跑就完事，还用费这劲。而且，才 10GB 的文件，也不算大啊。实在不行租台 r3.xlarge\t一小时 $0.350 ， 一下子就出来了。实在没钱，放 sqlite 里，做个 index, page_cache 有多少给多少，就硬算就完事.",
          "en": "@laike9m Because you deliberately want to eat shit. A reliable and simple solution would be great, but you insist on putting icing on the cake. This thing, just create a database and build an index, and it's done. Why bother with the effort? Besides, a 10GB file isn't that big. If that doesn't work, rent an r3.xlarge for $0.350 per hour, and it'll be done instantly. If you really have no money, put it in sqlite, create an index, give as much page_cache as you have, and just brute-force it."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "第一个问题就是你是否有足够的磁盘空间，如果有的话，完全可以先排完序再说。假设你使用 64 位操作系统，先分别排序两个 csv ，这样做：1. 把 x.csv 映射到虚拟内存。2. 扫描一次，计算行数 n 。3. 建立一个长度是 8n 字节的文件 x.dat ，映射到内存，把它看成长度是 n 的 uint64 数组 index 。4. 扫描 x.csv ，在 index[i] 放置第 (i-1) 行开始的位移。5. 对 index 的元素 z 按 x.csv 从 z 处提取出的字符串升序排序。6. 保存 x-sorted.csv 。上述操作需要 O(n log n) 的时间。然后同时把 a.csv, a.dat, b.csv, b.dat 映射到虚拟内存，并用有序合并算法计算需要的三个结果，这需要 O(n) 的时间。额外的磁盘空间复杂度是 O(n)，具体来说，显然不会超过 20 GB 。",
          "en": "The first question is whether you have enough disk space. If you do, you can sort the files first. Assuming you are using a 64-bit operating system, first sort the two csv files separately. To do this: 1. Map x.csv to virtual memory. 2. Scan once and calculate the number of lines n. 3. Create a file x.dat with a length of 8n bytes, map it to memory, and treat it as an uint64 array index of length n. 4. Scan x.csv and place the offset of the beginning of the (i-1)th line in index[i]. 5. Sort the elements of index z in ascending order based on the string extracted from z in x.csv. 6. Save x-sorted.csv. The above operations require O(n log n) time. Then, map a.csv, a.dat, b.csv, and b.dat to virtual memory simultaneously and use a merge sort algorithm to calculate the three results required, which requires O(n) time. The additional disk space complexity is O(n); specifically, it will obviously not exceed 20 GB."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "可以把 name 分组，比如先把两个文件中所有 a 开头的行读入内存比较，然后再比较 b 、c 。分组粒度大小按照内存大小来。",
          "en": "You can group by name, for example, read all the lines starting with 'a' in the two files into memory and compare them, and then compare 'b' and 'c'. The granularity of the grouping should be based on the size of the memory."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "不许排序是什么奇葩要求？那就不能导入数据库啊，数据库索引不算排序？那就不能用 hashmap 啊，hashmap 对数据计算 hash ，hashmap 本身就是对 hash 的排序啊坚持要处理无序数据，On^2 慢慢对呗，呵呵",
          "en": "Why the strange requirement of not allowing sorting? Then you can't import it into a database, doesn't a database index count as sorting? Then you can't use a hashmap, hashmap calculates the hash for the data, and the hashmap itself is a sorting of the hash. If you insist on processing unordered data, On^2, take your time, haha."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "你们没有数据开发吧，这思路太后端了OP 的最终需求就是校验 Oracle 迁移到 PostgreSQL 的数据，给了两个 CSV 是不能连数据库？考虑以下点：1. CSV 作为两边数据源的中间缓存，两边库导出的 CSV 就是错的，特殊字符转义等问题，这点就已经导致不一样；2. 校验任务执行频率和执行时间要求；3. 能否直连两边库；4. 中间缓存对两边库数据类型的兼容统一，只能 CSV 跳过这点；一次性比较我直接 cut sort comm ，写代码浪费生命。经常跑、对比文件就直接 导入 DuckDB FULL OUTER JOIN 。比较专业的方案 https://github.com/datafold/data-diff ，可以参考它的思路",
          "en": "You guys don't do data development, this thinking is too backend. The OP's ultimate requirement is to verify the data migrated from Oracle to PostgreSQL. Giving two CSVs means you can't connect to the database? Consider the following points: 1. CSV is used as an intermediate cache for data sources on both sides, and the CSVs exported from both databases are incorrect, such as special character escaping issues, which already cause differences; 2. The execution frequency and execution time requirements of the verification task; 3. Whether you can directly connect to the databases on both sides; 4. The compatibility and unification of the data types of the intermediate cache for the databases on both sides, and only CSV can skip this point; For a one-time comparison, I would directly use cut sort comm. Writing the code is a waste of life. If it runs frequently and compares files, just import DuckDB FULL OUTER JOIN directly. A more professional solution is https://github.com/datafold/data-diff, you can refer to its idea."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "@rocmax en,搜索过程给点反馈就能一次遍历产生三个结果. 1.有没有完全匹配 2.完全匹配年龄是否相同",
          "en": "@rocmax en, providing feedback during the search process enables you to generate the three results with a single traversal. 1. Whether there is a complete match 2. Whether the ages of the complete matches are the same"
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "这件事的正解就是排序，无非是你自己拍，还是数据库帮你排的区别。当然你有特殊癖好那就不在考虑范围了",
          "en": "The correct solution to this is sorting, it's just a matter of whether you sort it yourself or the database sorts it for you. Of course, if you have special preferences, that's out of consideration."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "能不能说下为什么不能用 sqlite ？爬完了也没看明白。。",
          "en": "Can you explain why you can't use sqlite? I didn't understand after reading all of it.."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "建议重点复习一下数据库 sort merge join 和 hash join 的实现，你说的这些问题在数据库领域是已知问题",
          "en": "I suggest you review the implementation of database sort merge join and hash join. The problems you mentioned are known issues in the database field."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "保持原来方案，但是思路换一换，10 组数据两两剔除相同元素。后续需要处理的数据量会越来越小。这样可行不。",
          "en": "Keep the original plan, but change the approach. Remove the same elements in pairs from the 10 groups of data. The amount of data that needs to be processed later will become smaller and smaller. Is this feasible?"
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "最讨厌这种，一开始不把事情说清楚，非得搞一些奇葩方案，再三逼问下，到了 64 楼才解释原始需求。导致一开始，此事就成为 X-Y 问题，浪费大家精力与时间。你这事，从软件工程角度来说，正确的解法，并不是找数据库去帮你排序，也不是自己写代码去处理，而是使用业界现有的数据验证工具。据我所知，数据治理领域，就存在不少的数据库互转与验证软件。你自己代码写得再正确，也不通用，不如学会使用这样的现成工具，减少浪费时间重复造轮子的事情。",
          "en": "I hate this the most, not making things clear at the beginning, and having to come up with some weird solutions. After repeated questioning, the original requirements were only explained on the 64th floor. As a result, at the beginning, this became an X-Y problem, wasting everyone's energy and time. From a software engineering point of view, the correct solution to this is not to ask the database to sort it for you, nor is it to write code to handle it yourself, but to use existing data verification tools in the industry. As far as I know, there are many database conversion and verification software in the data governance field. No matter how correct your own code is, it is not universal. It is better to learn to use such ready-made tools and reduce the waste of time in reinventing the wheel."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "看看这种方法行不  （假设 Name 支持任意字符）将文件分片1 ） 选取一个质数作为分片的值 （例如 977 ）2 ） 将 A 文件和 B 文件分片， 要保证相同的名字在相同的分片号， 且分片尽可能均匀我帮你想到的一个合理的办法：  取 Name 的 UTF8 。 如果 UTF8 长度不能被 4 整除，则添 0 将数组长度补成 4 的倍数。每四个字节映射为一个 int32 类型， 然后把这些 int32 加起来。 然后%977 （一个比较大的指数）。 这样会得到 0-966 中的一个值。 3 ） 你的问题化简成了在分片内的问题 （因为相同的名字对应相同的分片）",
          "en": "See if this method works (assuming Name supports any characters) to shard the file 1) Select a prime number as the shard value (e.g., 977) 2) Shard files A and B, ensuring that the same names are in the same shard number, and the shards are as evenly distributed as possible. A reasonable method I thought of: Take the UTF8 of Name. If the UTF8 length is not divisible by 4, add 0 to make the array length a multiple of 4. Map every four bytes to an int32 type, and then add up these int32s. Then %977 (a relatively large exponent). This will result in a value from 0-966. 3) Your problem is simplified into the problem within the shard (because the same names correspond to the same shard)"
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "我的第一感觉是,  这个事情应该导入的数据库表里再进行相关操作,写程序对比不是最佳方案. 10GB 单表在数据库里也不算多大.你要的那三个比较结果在 SQL 里都是很容易得到的.",
          "en": "My first impression is that this should be imported into the database table and then perform relevant operations; writing a program to compare them is not the best solution. A 10GB single table isn't that big in a database. The three comparison results you want are very easy to obtain in SQL."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "我觉着 ls 的大佬说的对，放 mysql 里就完事了。",
          "en": "I think the big guys above are right, just put it in mysql and it's done."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "@FlytoSirius @hefish 宝，很感激你们的回复。因为现在的目的就是用外部程序去验证两个数据库中的数据是否完全一致，且有种种条条框框加成下，只能用程序去读 CSV 去验证哈。",
          "en": "@FlytoSirius @hefish Buddy, I'm very grateful for your replies. Because the current purpose is to use an external program to verify whether the data in the two databases is completely consistent, and under various constraints, I can only use a program to read the CSVs to verify, haha."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "买内存条啊",
          "en": "Buy a memory stick"
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "@FeifeiJin 那能不能程序直接连接 两端数据库进行对比呢?    毕竟导出这样的两个数 GB 的大表是个容易出问题的过程.   但, 如果就得用程序去对比文件, 那确实要花点时间研究下具体实现.",
          "en": "@FeifeiJin Then, can the program directly connect to the two databases and compare them? After all, exporting such large tables of several GB is a process prone to problems. But, if you have to use the program to compare the files, then you really need to spend some time studying the specific implementation."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "@FlytoSirius 真的不能。",
          "en": "@FlytoSirius Really can't."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "@min 谢谢您的回复。",
          "en": "@min Thank you for your reply."
        }
      ],
      "type": "replies"
    },
    {
      "chunks": [
        {
          "zh": "如果是 Java 可以试试阿里的 easyexcel ，https://github.com/alibaba/easyexcel",
          "en": "If it is Java, you can try Alibaba's easyexcel, https://github.com/alibaba/easyexcel"
        }
      ],
      "type": "replies"
    }
  ]
}